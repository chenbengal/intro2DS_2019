{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW9: Boosting and Neural Nets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## instructions\n",
    "\n",
    "our course will be using an automatic grading system. <br>\n",
    "after each question there will appear a code block with some prepared code to add your answer to a dictionary that will be sent to the course server for grading. <br>\n",
    "please do not edit any code other than in placeholders marked `#### your code here ####` <br>\n",
    "__don't forget to run the code block after you write your answer.__\n",
    "\n",
    "\n",
    "you can add code blocks wherever you want in order to interact with datasets and play with your own code. <br>\n",
    "in the next code block plase fill in your id number and email account in the appropriate placees. <br>\n",
    "and __don't forget to run the block!__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = {}\n",
    "ans['HW'] = 'HW9'\n",
    "ans['id_number'] = #### your id here ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1)\n",
    "We described gradient boosting as taking the (negative) derivative of the loss after $t-1$ iterations and using that as the response for finding the weak learner in iteration $t.$ We than explained the use of residual as (half) the negative derivative of squared loss $L(y,\\hat{y}) = (y-\\hat{y})^2$. Using the notation from class, where $F^{(t-1)}$ is the model after $t-1$ iterations, we can denote this:\n",
    "$$ y^{(t)}_i = y_i - F^{(t-1)}(x_i).$$\n",
    "\n",
    "Now assume we instead do the following, for some $c>0$: \n",
    "$$ y^{(t)}_i = \\left\\{\\begin{array}{ll} y_i - F^{(t-1)}(x_i) & \\mbox{if } |y_i - F^{(t-1)}(x_i)|<c\\\\\n",
    "c & \\mbox{if } y_i - F^{(t-1)}(x_i) \\geq c\\\\\n",
    "-c  & \\mbox{if } y_i - F^{(t-1)}(x_i) \\leq -c. \\end{array} \\right.$$\n",
    "\n",
    "What is a description of the loss function $L(y,\\hat{y})$ that the resulting gradient boosting algorithm uses? \n",
    "\n",
    "1. It is quadratic when $|y-\\hat{y}|<c$ and fixed when $|y-\\hat{y}|\\geq c$\n",
    "2. It is absolute loss $L(y,\\hat{y}) = |y-\\hat{y}|$\n",
    "3. It is quadratic when $|y-\\hat{y}|<c$ and continues linearly when $|y-\\hat{y}|\\geq c$\n",
    "4. There is no corresponding convex loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans['Q1'] = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q2)\n",
    "Assume we build a Neural Network for regression as described in class, with one or more hidden layers, with one or more nodes in each hidden layer, and nonlinear transformation $\\sigma$. For example, a model with two hidden layers, and two nodes in each, and $p$ explanatory variables, will be (as we showed in class): \n",
    "$$ f(x) = \\sum_{i=1}^2 w_i^{(3)} \\sigma \\left(\\sum_{j=1}^2 w^{(2)}_{ji} \\sigma (\\sum_{k=1}^p w^{(1)}_{kj} x_k)\\right).$$\n",
    "Assume that for the \"nonlinearity\" function $\\sigma$ we decide to actually use the identity function $\\sigma(u)=u.$\n",
    "What can we say about the resulting model?\n",
    "\n",
    "1. The model is linear in the covariates, i.e., $f(x) = \\beta^t x$ for some $\\beta$, and therefore we have gained nothing from the complex neural net\n",
    "2. The resulting model will always be the identity $f(x)=x$, and therefore it is meaningless to use this function\n",
    "3. The model we get can be a complex and non-linear function of $x$, because of the large number of weights in the network\n",
    "4. The choice of $\\sigma$ can be anything we want, including the zero function $\\sigma(u)=0,\\; \\forall u$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans['Q2'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q3)\n",
    "Assume we run an iterative boosting algorithm, as described in class, but update the \"residual\" function $Y^{(t)}$ not every iteration, but every 10 iterations, for example if we use the actual residual we now get: \n",
    "\n",
    "1. Initialize $F^{(0)}(x) = 0\\; \\forall x,\\;,Y^{(0)}=Y$\n",
    "2. At stage $t \\geq 1$:<br>    \n",
    "    a. If $(t mod 10) == 0$ update $y_i^{(t)} = y_i - F^{(t-1)}(x_i),\\;\\forall i,$ otherwise set $y_i^{(t)} = y_i^{(t-1)},\\;\\forall i$ the previous value<br>\n",
    "    b. Fit a weak learner $\\hat{f}^{(t)}$ to $T^{(t)} = (X,Y^{(t)})$<br>\n",
    "    c. Update $F^{(t)} = F^{(t-1)} + \\epsilon \\hat{f}^{(t)}$\n",
    "\n",
    "What will this algorithm do after $T$ iterations? In particular, can we get the same model with an algorithm that updates the residual at every iteration? \n",
    "\n",
    "1. The same as building a model with $10$ times bigger data, with the same values of $\\epsilon$ and $T$, and updating the residuals every iteration \n",
    "2. The same as building a model with $10 \\epsilon$ (ten times bigger) for $T/10$ iterations, and updating the residuals every iteration\n",
    "3. The same as building a model with $\\epsilon/10$ (ten times smaller) for $10T$ iterations, and updating the residuals every iteration\n",
    "4. There is no analogous approach that updates at every iteration\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans['Q3'] = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q4)\n",
    "Consider a very simple Neural network, applied to data with $p=2$ covariates, one hidden layer with one hidden unit with nonlinearity (activation) $\\sigma(u) = 1/1(1+exp(-u))$, and no non-linearity in the output layer.<br>Note: This function has the nice derivative $\\frac{\\partial \\sigma(u)}{\\partial u} = \\sigma(u) (1-\\sigma(u)).$\n",
    "\n",
    "This network has a total of $2+1$ weights. Assume we use the squared loss (RSS) to train the network. We have a current set of weights $W = (w^{(1)}_1, w^{(1)}_2, w^{(2)})$, and want to calculate the derivative $$\\frac{\\partial (y_i - \\hat{y}_i)^2}{\\partial w^{(1)}_1},$$ with \n",
    "$$ \\hat{y}_i = w^{(2)}  \\sigma (w^{(1)}_1 x_{i1} + w^{(1)}_2 x_{i2}).$$\n",
    "\n",
    "Use the chain rule to find the correct expression for this derivative (denote $z = w^{(1)}_1 x_{i1} + w^{(1)}_2 x_{i2}$):\n",
    "1. $-2 (y-\\hat{y}_i) \\sigma(z) (1-\\sigma(z))$\n",
    "2. $-2 (y-\\hat{y}_i) w^{(2)} \\sigma(z) (1-\\sigma(z)) x_{i1}$\n",
    "3. $ w^{(2)} \\sigma(z) $\n",
    "4. $ w^{(2)} \\sigma(z) (1-\\sigma(z)) + \\sigma(z) (1-\\sigma(z)) x_{i1}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans['Q4'] = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSD 5.5: boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0. For this Case Study assignment you should have in your current folder the ebay_boys_girls_shirts folder, holding the four CSV files describing the train and test shirts images, and the boys and girls images folders. This is what we did in CSD 1, **if you already have the data in your current folder you don't need to run this again!**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import tarfile\n",
    "\n",
    "url = \"http://www.tau.ac.il/~saharon/DScourse/ebay_boys_girls_shirts.tar.gz\"\n",
    "r = requests.get(url)\n",
    "\n",
    "with open(\"ebay_boys_girls_shirts.tar\", \"wb\") as file:\n",
    "    file.write(r.content)\n",
    "\n",
    "with tarfile.open(\"ebay_boys_girls_shirts.tar\") as tar:\n",
    "    tar.extractall('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. In CSD4 we got the `x_train` and `x_test` matrices, and the `y_train` and `y_test` 0/1 vectors (0 = boys, 1 = girls).\n",
    "\n",
    "    Complete the `get_final_matrices` function using all functions we've composed, to quickly get the four elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import transform, color, img_as_ubyte\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def get_file_list(df, folder, n_sample = None, seed = None):\n",
    "    if n_sample is None:\n",
    "        file_ids_list = df.file_id.values\n",
    "    else:\n",
    "        file_ids_list = df.sample(n = n_sample, random_state = seed).file_id.values\n",
    "    files_list = [folder + '/' + str(file_id) + '.jpg' for file_id in file_ids_list]\n",
    "    return files_list\n",
    "\n",
    "def read_image_and_resize(f, w = 100, h = 100):\n",
    "    img = plt.imread(f)\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        img = transform.resize(img, (w, h), mode='constant')\n",
    "        img = img_as_ubyte(img)\n",
    "    img = color.gray2rgb(img)\n",
    "    img = img[np.newaxis, :, :, :3]\n",
    "    if img.shape != (1, 100, 100, 3):\n",
    "        raise ValueError(f + str(img.shape))\n",
    "    return img\n",
    "\n",
    "def read_images_4d_array(files_list):\n",
    "    images_list = [read_image_and_resize(file) for file in files_list]\n",
    "    images_array = np.concatenate(images_list)\n",
    "    return images_array\n",
    "\n",
    "def get_images_matrix(csv_file, folder, n = None, seed = 1976):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    files_list = get_file_list(df, folder, n, seed)\n",
    "    images = read_images_4d_array(files_list)\n",
    "    return images, files_list\n",
    "\n",
    "def get_all_pixels(x):\n",
    "    return x.reshape(-1, np.prod(x.shape[1:]))\n",
    "\n",
    "def numpy_array_size_in_bytes(a):\n",
    "    return a.size * a.itemsize\n",
    "\n",
    "def shape_and_size(x, name):\n",
    "    n_rows = x.shape[0]\n",
    "    if len(x.shape) == 1:\n",
    "        n_cols = 1\n",
    "    elif len(x.shape) == 2:\n",
    "        n_cols = x.shape[1]\n",
    "    else:\n",
    "        warnings.warn('Function is meaningful for 1 or 2-D numpy arrays, taking 2nd dimension as n_cols')\n",
    "        n_cols = x.shape[1]        \n",
    "    size = numpy_array_size_in_bytes(x)\n",
    "    print('%s Shape: %d X %d, Size (bytes): %d' % (name, n_rows, n_cols, size))\n",
    "\n",
    "def conf_matrix(y_true, y_pred):\n",
    "    return pd.crosstab(y_true, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)\n",
    "\n",
    "def get_final_matrices(n_train = None, n_test = None):\n",
    "    folder = 'ebay_boys_girls_shirts/'\n",
    "    x_boys_train, boys_train_files = get_images_matrix(folder + 'boys_train.csv', folder + 'boys', n_train)\n",
    "    x_boys_test, boys_test_files = get_images_matrix(folder + 'boys_test.csv', folder + 'boys', n_test)\n",
    "    x_girls_train, girls_train_files = get_images_matrix(folder + 'girls_train.csv', folder + 'girls', n_train)\n",
    "    x_girls_test, girls_test_files = get_images_matrix(folder + 'girls_test.csv', folder + 'girls', n_test)\n",
    "    \n",
    "    x_boys_train_all = get_all_pixels(x_boys_train)\n",
    "    x_boys_test_all = get_all_pixels(x_boys_test)\n",
    "    x_girls_train_all = get_all_pixels(x_girls_train)\n",
    "    x_girls_test_all = get_all_pixels(x_girls_test)\n",
    "\n",
    "    x_train = np.vstack([x_boys_train_all, x_girls_train_all])\n",
    "    x_test = np.vstack([x_boys_test_all, x_girls_test_all])\n",
    "\n",
    "    y_boys_train = np.array([np.uint8(0)] * x_boys_train.shape[0])\n",
    "    y_boys_test = np.array([np.uint8(0)] * x_boys_test.shape[0])\n",
    "    y_girls_train = np.array([np.uint8(1)] * x_girls_train.shape[0])\n",
    "    y_girls_test = np.array([np.uint8(1)] * x_girls_test.shape[0])\n",
    "    y_train = np.concatenate([y_boys_train, y_girls_train])\n",
    "    y_test = np.concatenate([y_boys_test, y_girls_test])\n",
    "    \n",
    "    return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload 20% of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = get_final_matrices(n_train = 2000, n_test = 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you got what you wanted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_and_size(x_train, 'x_train')\n",
    "shape_and_size(x_test, 'x_test')\n",
    "shape_and_size(y_train, 'y_train')\n",
    "shape_and_size(y_test, 'y_test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. now let's try Gradient Boosted Trees with 100 trees. <br>\n",
    "**this may take a while, If you're short on time try it with 10 trees at first** <br>\n",
    "do not forget to use a random seed (random_state) of 0, this ensures reproducability of the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "mod = GradientBoostingClassifier(n_estimators=100,random_state=0)\n",
    "\n",
    "\n",
    "acc = mod.score(x_test, y_test)\n",
    "ans['Q5'] = acc\n",
    "print('Test accuracy on all pixels with GBT: %.3f' % acc)\n",
    "\n",
    "y_pred = mod.predict(x_test)\n",
    "y_test_s = np.array(['boys'] * len(y_test))\n",
    "y_pred_s = np.array(['boys'] * len(y_test))\n",
    "y_test_s[y_test == 1] = 'girls'\n",
    "y_pred_s[y_pred == 1] = 'girls'\n",
    "conf_matrix(y_test_s, y_pred_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. as usual in classification problems, the final model depends on the threshold for classification. it is not always best to use the intuitive 0.5 threshold. <br>\n",
    "plot the roc and report the auc of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "y_pred_prob = mod.predict_proba(x_test)\n",
    "y_pred_prob_1d = y_pred_prob[:, 0]\n",
    "\n",
    "# Compute fpr, tpr, thresholds and roc auc\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob_1d, pos_label=0)\n",
    "roc_auc = roc_auc_score(1 - y_test, y_pred_prob_1d)\n",
    "ans['Q6'] = roc_auc\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.plot(fpr, tpr, label='ROC curve (area = %0.3f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], 'k--')  # random predictions curve\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## neural network\n",
    "as shown in the recitation create a simple fully connected neural network for classification.\n",
    "use one hidden layer with 500 nodes and a \"tanh\" activation function.\n",
    "train it for 50 epochs with a batch size of 100, use the binary cross entropy loss and adam solver as demonstrated in the recitation. <br>\n",
    "remember the input layer size needs to be equal to the number of parameters, and you will want to use  an output layer of size 1 with a sigmoid activation function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 500)               15000000  \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 500       \n",
      "=================================================================\n",
      "Total params: 15,000,500\n",
      "Trainable params: 15,000,500\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(2)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(500, input_dim=30000, activation='tanh', use_bias=False))\n",
    "model.add(Dense(1, activation='sigmoid', use_bias=False))\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15000500"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_weights = 30000*500 + 500*1\n",
    "print(num_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "report the following:\n",
    "\n",
    "__Q7)__ how many weights does this network need to learn?<br>\n",
    "__Q8)__ what is the AUC of this model?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans['Q7'] = #### your answer here ####\n",
    "ans['Q8'] = #### your answer here ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bonus\n",
    "\n",
    "play with the neural network hyper-parameters (depth, # nodes, activation function, batch size, # epochs etc.)<br>\n",
    "report your models AUC.<br>\n",
    "you will get 5 points for beating the auc of the previous simple model<br>\n",
    "and 5 extra points for the student with the best AUC in class.<br>\n",
    "__do not__ change the random seed values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(2)\n",
    "\n",
    "### your code here ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "report your best auc score, and describe the architecture of your model in free text (keep it short and informative, we will reporoduce your settings to validate the result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans['bonus'] = ### auc score ###\n",
    "ans['bonus_model'] = ### model ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# finish!\n",
    "\n",
    "to submit your HW please run this last code block and follow the instructions. <BR>\n",
    "this code will create a CSV file in the current directory on the azure notebooks project <br>\n",
    "please download it and submit it through moodle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_ans = pd.DataFrame.from_dict(ans, orient='index')\n",
    "if df_ans.shape[0] == 10 or df_ans.shape[0] == 12:\n",
    "    df_ans.to_csv('{}_{}.csv'.format(ans['HW'],str(ans['id_number'])))\n",
    "    print(\"OK!\")\n",
    "else:\n",
    "    print(\"seems like you missed a question, make sure you have run all the code blocks\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
