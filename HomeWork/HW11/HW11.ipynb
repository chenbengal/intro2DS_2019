{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 11: Big Data Computing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## instructions\n",
    "\n",
    "our course will be using an automatic grading system. <br>\n",
    "after each question there will appear a code block with some prepared code to add your answer to a dictionary that will be sent to the course server for grading. <br>\n",
    "please do not edit any code other than in placeholders marked `#### your code here ####` <br>\n",
    "__don't forget to run the code block after you write your answer.__\n",
    "\n",
    "\n",
    "you can add code blocks wherever you want in order to interact with datasets and play with your own code. <br>\n",
    "in the next code block plase fill in your id number and email account in the appropriate placees. <br>\n",
    "and __don't forget to run the block!__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = {}\n",
    "ans['HW'] = 'HW11'\n",
    "ans['id_number'] = #### your id here ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1)\n",
    "Given a matrix of $M\\times14$ of grades for $14$ movies from $M$ users, you want to calculate the average grade of each movie using MapReduce. You are given a reducer function which gets a $(movie,[values])$ pair and simply sums all the elements of the $[values]$ list it receives. \n",
    "You divide the data into $n$ files of size $m=M/n$ rows each, and each mapper gets a $m\\times 14$ matrix of ratings $X$, what should this mapper do? \n",
    "\n",
    "1. Multiply $X$ by a column vector of length $14$ and value $1/M,$ return $14$ $(movie, value)$ pairs\n",
    "2. Multiply $X$ by a column vector of length $14$ and value $1/m,$ return $14$ $(movie, value)$ pairs\n",
    "3. Sum each row of $X$, return $m$ $(user, value)$ pairs\n",
    "4. Average each row of $X$, return $m$ $(user, value)$ pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans['Q1'] = #### your answer here ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q2)\n",
    "Which of the following is not typically an advantage of MapReduce over centralized computation on the entire dataset?\n",
    "\n",
    "1. That MapReduce can accomodate huge datasets that do not fit in memory\n",
    "2. That MapReduce requires fewer computations overall to calcualte the same quantity\n",
    "3. That MapReduce can take advantage of having distributed memory and computation resources\n",
    "4. That MapReduce is more fault tolerant "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans['Q2'] = #### your answer here ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q3)\n",
    "We mentioned in class that calculating a median of a huge list of $M$ numbers is not readily amenable for MapReduce. An approximation (not necessarily a good one), given $n$ files of size $m=M/n$ each, is to calculate the median of each file, and then the median of these medians as the estimate of the overall median. Which of the following pairs of map and reduce Python functions implements this simple idea?\n",
    "\n",
    "1.     \n",
    "            def mapper(list_of_nums,file_num):\n",
    "                list_of_key_values = [(file_num, median(list_of_nums))]\n",
    "                return list_of_key_values       \n",
    "            def reducer(single_key_values):\n",
    "                return median(single_key_values)\n",
    "\n",
    "2.     \n",
    "            def mapper(list_of_nums,file_num):\n",
    "                list_of_key_values = [(file_num, median(list_of_nums))]\n",
    "                return list_of_key_values       \n",
    "            def reducer(single_key_values):\n",
    "                key, values = single_key_values\n",
    "                return key, median(values)\n",
    "\n",
    "3.     \n",
    "            def mapper(list_of_nums):\n",
    "                list_of_key_values = [('whatever', median(list_of_nums))]\n",
    "                return list_of_key_values       \n",
    "            def reducer(single_key_values):\n",
    "                key, values = single_key_values\n",
    "                return key, median(values)\n",
    "\n",
    "4.     \n",
    "            def mapper(list_of_nums):\n",
    "                list_of_key_values = [(median(list_of_nums), median(list_of_nums))]\n",
    "                return list_of_key_values       \n",
    "            def reducer(single_key_values):\n",
    "                key, values = single_key_values\n",
    "                return key, median(values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans['Q3'] = #### your answer here ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q4)\n",
    "Now we need to figure out if the median of medians is a good approximation of the overall median of the big file. Assuming an adversary organizes the $M$ samples into $n$ files in the worst possible way for our algorithm, what is the minimal proportion of the original $M$ samples that will be smaller than our estimated \"median\"?\n",
    "\n",
    "1. Arbitrarily close to $0$\n",
    "2. About $1/4$\n",
    "3. About $1/3$\n",
    "4. If $M$ is big enough, close to $1/2$: this is a good estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans['Q4'] = #### your answer here ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you are the lead data scientist in a large company, you have vast amounts of data to process and managment needs your advice to choose an apropriate big data processing framework. \n",
    "\n",
    "__Q5)__ which frameworks processes data faster?\n",
    "1. hadoop mapreduce\n",
    "2. apache spark\n",
    "3. HDFS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans['Q5'] = #### your answer here ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q6)__ which frameworks processes data cheeper?\n",
    "1. hadoop mapreduce\n",
    "2. apache spark\n",
    "3. HDFS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans['Q6'] = #### your answer here ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q7)__ your company's product is a sofisticated algorithm used to predict the results of the eurovision contest using data from past contests. you must submit your predictions before the competition starts. which frame work is more suitable?\n",
    "1. hadoop mapreduce\n",
    "2. apache spark\n",
    "3. HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans['Q7'] = #### your answer here ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q8)__ your company has done so well on predicting eurovision results that executives have decided to use the technology for day trading in the stock market, which frame work is more suitable?\n",
    "1. hadoop mapreduce\n",
    "2. apache spark\n",
    "3. HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans['Q8'] = #### your answer here ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pyspark\n",
    "the following questions are regarding simple manipulations on the spark rdd data structure.<br>\n",
    "if you need a refresher on python lambda functions this [link](https://www.geeksforgeeks.org/python-lambda-anonymous-functions-filter-map-reduce/) could be usefull<br>\n",
    "also this [short tutorial](https://www.tutorialspoint.com/pyspark/pyspark_rdd.htm) on pysprk rdd can help.<br>\n",
    "and of course you should look back at the lecture slides and notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q9) What will this `mystery` function return? Assume `rdd` is an a simple RDD of numeric values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext()\n",
    "\n",
    "def mystery(rdd):\n",
    "    a = rdd.count()\n",
    "    \n",
    "    b = rdd.sortBy(lambda x: x) \\\n",
    "        .zipWithIndex() \\\n",
    "        .map(lambda x: (x[1], x[0]))\n",
    "    \n",
    "    c = a//2\n",
    "    \n",
    "    d = b.filter(lambda x: x[0] == c).collect()[0][1]\n",
    "    \n",
    "    if a % 2 == 1:\n",
    "        return d\n",
    "    else:\n",
    "        e = b.filter(lambda x: x[0] == c - 1).collect()[0][1]\n",
    "        return (d + e)/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. If the RDD is odd, its first value. If it is even, the average of its first and last values\n",
    "2. If the RDD is odd, its smallest value. If it is even, the average of its smallest and largest values\n",
    "3. If the RDD is odd, its largest value. If it is even, the average of its largest value and the next largest value\n",
    "4. The RDD median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans['Q9'] = #### your answer here ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q10)__ A function receives a RDD of 2-tuples, e.g. `sc.parallelize([(1, 3),(2, 1),(3, 1)])`.\n",
    "\n",
    "The first element in the tuple is the value, and the second is its frequency, e.g. \"1 appears 3 times, 2 appears 1 time, etc.\"\n",
    "\n",
    "Which of the following will compute the harmonic mean of the ungrouped values?\n",
    "\n",
    "For example, the harmonic mean of the above RDD is: $5/(1/1 + 1/1 + 1/1 + 1/2 + 1/3) = 1.304$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def a(rdd):\n",
    "    return rdd.count()/rdd.map(lambda x: 1/x).reduce(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def b(rdd):\n",
    "    return rdd.map(lambda x: x[1]).sum()/rdd.map(lambda x: (1/x[0], x[1])).map(lambda x: x[0]*x[1]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def c(rdd):\n",
    "    return rdd.map(lambda x: x[1]).count() / rdd.map(lambda x: x[1]/x[0]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d(rdd):\n",
    "    return rdd.count()/rdd.mapValues(lambda v: 1/v).map(lambda x: x[1]).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "give your answer as a string of the function name (example: ans['Q10'] =\"e\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans['Q10'] = #### your answer here ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# finish!\n",
    "\n",
    "to submit your HW please run this last code block and follow the instructions. <BR>\n",
    "this code will create a CSV file in the current directory on the azure notebooks project <br>\n",
    "please download it and submit it through moodle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_ans = pd.DataFrame.from_dict(ans, orient='index')\n",
    "if df_ans.shape[0] == 12:\n",
    "    df_ans.to_csv('{}_{}.csv'.format(ans['HW'],str(ans['id_number'])))\n",
    "    print(\"OK!\")\n",
    "else:\n",
    "    print(\"seems like you missed a question, make sure you have run all the code blocks\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
