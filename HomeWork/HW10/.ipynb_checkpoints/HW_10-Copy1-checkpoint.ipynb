{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW10: Convolutional NN's, and back to predictive modeling basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## instructions\n",
    "\n",
    "our course will be using an automatic grading system. <br>\n",
    "after each question there will appear a code block with some prepared code to add your answer to a dictionary that will be sent to the course server for grading. <br>\n",
    "please do not edit any code other than in placeholders marked `#### your code here ####` <br>\n",
    "__don't forget to run the code block after you write your answer.__\n",
    "\n",
    "\n",
    "you can add code blocks wherever you want in order to interact with datasets and play with your own code. <br>\n",
    "in the next code block plase fill in your id number and email account in the appropriate placees. <br>\n",
    "and __don't forget to run the block!__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = {}\n",
    "ans['HW'] = 'HW10'\n",
    "ans['id_number'] = #### your id here ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1)\n",
    "We described a CNN as a sequence of layers, consisting of convolution layers and max-pooling layers (and possibly others like dropout), and at the end a flattening and fully connected layer. \n",
    "Assume we have two versions of the same image, call them $A$ and $B$ with a shift between them of a few pixels: $A(i,j) = B(i+k_1, j+k_2),\\; \\forall i,j$ (with appropriate \"neutral\" padding at the edges to make the images of the same size, which does not have features that correspond to convolution filters). \n",
    "Suppose we run both through the same network (after training) that contains several pairs of convlution+max pooling layers, and takes this to extreme by ending up with a large number $K$ of tiny $1\\times 1$ images. What is likely to be true of these images?\n",
    "\n",
    "1. The $K$ values will be the same for $A,B$, but not in the same order\n",
    "2. Because the images are shifted, the eventual tiny convolution images will be different\n",
    "3. The $K$ values will likely be identical for both initial images\n",
    "4. The eventual images will be different, even if there is no shift ($k_1=k_2=0$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans['Q1'] = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q2)\n",
    "In the CNN we saw in class, after each convolution layer there was a non-linearity \"layer\" (meaning a non-linear transformation was applied to the resulting convolution images before the next layer). In this example it was relu:\n",
    "$$ relu(x) = x^+ = max(x,0).$$\n",
    "Assume the following: we have a grey-level image (white=0, black=255) with a black vertical line on a white background, and we apply to it a filter that searches for a vertical line (similar to what  we showed in class):\n",
    "$$ f = \\left( \\begin{array}{ccc} -0.5 & 1 & -0.5 \\\\ -0.5 & 1 & -0.5 \\\\ -0.5 & 1 & -0.5 \\end{array} \\right).$$\n",
    "We then do a relu transformation on the resulting image, then max-pooling, and another identical filter for a vertical line. Will the resulting image have a dark vertical line in it?  \n",
    "\n",
    "1. Yes, because the non-linearity is monotone and maintains the dark line\n",
    "2. No, because the non-linearity changes the meaning of the image\n",
    "3. Yes, because this filter creates convolution images with dark lines regardless of what is in the original image\n",
    "4. No, because this filter destroys the lines in the original image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans['Q2'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q3)\n",
    "Which of the following **does not** add flexibility to the model?\n",
    "\n",
    "1. Adding variables to an OLS model\n",
    "2. Increasing the depth of a tree model\n",
    "3. Adding more trees (iterations) to a boosting model\n",
    "4. Adding more trees (iterations) to a Random Forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans['Q3'] = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q4)\n",
    "Assume that instead of dividing our data into training and test set as we did in class, we divide it into three parts: \n",
    "* train (say 60%)\n",
    "* validation (say 20%)\n",
    "* test (20%)\n",
    "We train several different models on the training set, choose between them based on validation set performance, and apply the winner to the test set. The problem can be regression or classification, and the error measure can be squared loss, misclassification erorr, or any other relevant measure. \n",
    "Denote the prediction error on train by $e_{tr}$, and respectively $e_{va}$ and $e_{te}$. What is the typical order between the errors of the chosen model on the different sets ($\\approx$ means approximately equal or typically similar):\n",
    "\n",
    "1. $e_{tr} \\leq e_{va} \\leq e_{te}$\n",
    "2. $e_{tr} \\approx e_{va} \\leq e_{te}$\n",
    "3. $e_{tr} \\leq e_{va} \\approx e_{te}$\n",
    "4. $e_{tr} \\approx e_{va} \\approx e_{te}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans['Q4'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSD 7: Deep Learning for Image Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0. For this Case Study assignment you should have in your current folder the ebay_boys_girls_shirts folder, holding the four CSV files describing the train and test shirts images, and the boys and girls images folders. This is what we did in CSD 1, **if you already have the data in your current folder you don't need to run this again!**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import tarfile\n",
    "\n",
    "url = \"http://www.tau.ac.il/~saharon/DScourse/ebay_boys_girls_shirts.tar.gz\"\n",
    "r = requests.get(url)\n",
    "\n",
    "with open(\"ebay_boys_girls_shirts.tar\", \"wb\") as file:\n",
    "    file.write(r.content)\n",
    "\n",
    "with tarfile.open(\"ebay_boys_girls_shirts.tar\") as tar:\n",
    "    tar.extractall('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. In this Case Study assignment we again try to classify an unseen shirt image as being of \"boys\" or of \"girls\". Yet this time we will be using heavier machinery: Deep Learning. Specifically we're going to get good results using Convolutional Neural Networks.\n",
    "\n",
    "    But first, load all matrices with the help of the functions composed so far:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import transform, color, img_as_ubyte\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def get_file_list(df, folder, n_sample = None, seed = None):\n",
    "    if n_sample is None:\n",
    "        file_ids_list = df.file_id.values\n",
    "    else:\n",
    "        file_ids_list = df.sample(n = n_sample, random_state = seed).file_id.values\n",
    "    files_list = [folder + '/' + str(file_id) + '.jpg' for file_id in file_ids_list]\n",
    "    return files_list\n",
    "\n",
    "def read_image_and_resize(f, w = 100, h = 100):\n",
    "    img = plt.imread(f)\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        img = transform.resize(img, (w, h), mode='constant')\n",
    "        img = img_as_ubyte(img)\n",
    "    img = color.gray2rgb(img)\n",
    "    img = img[np.newaxis, :, :, :3]\n",
    "    if img.shape != (1, 100, 100, 3):\n",
    "        raise ValueError(f + str(img.shape))\n",
    "    return img\n",
    "\n",
    "def read_images_4d_array(files_list):\n",
    "    images_list = [read_image_and_resize(file) for file in files_list]\n",
    "    images_array = np.concatenate(images_list)\n",
    "    return images_array\n",
    "\n",
    "def get_images_matrix(csv_file, folder, n = None, seed = 1976):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    files_list = get_file_list(df, folder, n, seed)\n",
    "    images = read_images_4d_array(files_list)\n",
    "    return images, files_list\n",
    "\n",
    "def get_all_pixels(x):\n",
    "    return x.reshape(-1, np.prod(x.shape[1:]))\n",
    "\n",
    "def numpy_array_size_in_bytes(a):\n",
    "    return a.size * a.itemsize\n",
    "\n",
    "def shape_and_size(x, name):\n",
    "    n_rows = x.shape[0]\n",
    "    if len(x.shape) == 1:\n",
    "        n_cols = 1\n",
    "    elif len(x.shape) == 2:\n",
    "        n_cols = x.shape[1]\n",
    "    else:\n",
    "        warnings.warn('Function is meaningful for 1 or 2-D numpy arrays, taking 2nd dimension as n_cols')\n",
    "        n_cols = x.shape[1]        \n",
    "    size = numpy_array_size_in_bytes(x)\n",
    "    print('%s Shape: %d X %d, Size (bytes): %d' % (name, n_rows, n_cols, size))\n",
    "\n",
    "def conf_matrix(y_true, y_pred):\n",
    "    return pd.crosstab(y_true, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)\n",
    "\n",
    "def get_final_matrices(n_train = None, n_test = None):\n",
    "    folder = 'ebay_boys_girls_shirts/'\n",
    "    x_boys_train, boys_train_files = get_images_matrix(folder + 'boys_train.csv', folder + 'boys', n_train)\n",
    "    x_boys_test, boys_test_files = get_images_matrix(folder + 'boys_test.csv', folder + 'boys', n_test)\n",
    "    x_girls_train, girls_train_files = get_images_matrix(folder + 'girls_train.csv', folder + 'girls', n_train)\n",
    "    x_girls_test, girls_test_files = get_images_matrix(folder + 'girls_test.csv', folder + 'girls', n_test)\n",
    "    \n",
    "    x_boys_train_all = get_all_pixels(x_boys_train)\n",
    "    x_boys_test_all = get_all_pixels(x_boys_test)\n",
    "    x_girls_train_all = get_all_pixels(x_girls_train)\n",
    "    x_girls_test_all = get_all_pixels(x_girls_test)\n",
    "\n",
    "    x_train = np.vstack([x_boys_train_all, x_girls_train_all])\n",
    "    x_test = np.vstack([x_boys_test_all, x_girls_test_all])\n",
    "\n",
    "    y_boys_train = np.array([np.uint8(0)] * x_boys_train.shape[0])\n",
    "    y_boys_test = np.array([np.uint8(0)] * x_boys_test.shape[0])\n",
    "    y_girls_train = np.array([np.uint8(1)] * x_girls_train.shape[0])\n",
    "    y_girls_test = np.array([np.uint8(1)] * x_girls_test.shape[0])\n",
    "    y_train = np.concatenate([y_boys_train, y_girls_train])\n",
    "    y_test = np.concatenate([y_boys_test, y_girls_test])\n",
    "    \n",
    "    return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = get_final_matrices()\n",
    "\n",
    "shape_and_size(x_train, 'x_train')\n",
    "shape_and_size(x_test, 'x_test')\n",
    "shape_and_size(y_train, 'y_train')\n",
    "shape_and_size(y_test, 'y_test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our platform of choice [Keras](https://keras.io/) accepts `x_train` of type `float`. It's best to turn it to float in the 0-1 range, the size should still be OK (or you'll need to add more memory to your Docker/Azure machine):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. We'll start with a \"simple\" neural network with a 6 hidden (`Dense`) layers, a.k.a a Multi-Layered Perceptron.\n",
    "    the hidden layer should have the following number of nodes (in decenting order) [512, 256, 128, 64, 32, 16]\n",
    "    We'll use a standard batch size of 128, for 10 epochs, a RELU activation at each hidden layer and Dropout rate of 0.2  between layers. Since we're looking for a single 0-1 probability-like score to classify a shirt image as of \"boys\" or of \"girls\" we'll use the `sigmoid` activation function in the final layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "\n",
    "batch_size = ### your code here ###\n",
    "epochs = ### your code here ###\n",
    "model = ### your code here ###\n",
    "\n",
    "### your code here ###\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here our optimizer method of choice will be [`adam`](https://keras.io/optimizers/#adam) and the loss function [`binary_crossentropy`](https://keras.io/backend/#binary_crossentropy). This is how we `compile` the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this is the actual `fit`ting of the model on `x_train` and `y_train`, having `x_test` and `y_test` as validation data (as always, best put here a dataset *different* than the absolutely final test dataset).\n",
    "\n",
    "**WARNING**: This is the part which takes long time, depending on hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    shuffle=True,\n",
    "                    validation_data=(x_test, y_test))\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=1)\n",
    "\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We reached a not so impressive accuracy of ~73%. If we kept `history` we can plot the model's performance through the different epochs and we can guesstimate if there's any point in having more epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc = 'upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q5)__ how many weights does this network need to learn?<br>\n",
    "__Q6)__ what is the AUC of this model?<br>\n",
    "__Q7)__ what is the avg F1 score of this model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans['Q5'] = #### your answer here ####\n",
    "ans['Q6'] = #### your answer here ####\n",
    "ans['Q7'] = #### your answer here ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. And now for Convolutional Neural Networks.\n",
    "\n",
    "    Here Keras needs the original 4D shape of the images array, so we `reshape` them to be of dimensions [N images X Height X Width X N channels].\n",
    "    We're using a `Conv2D` layer of 32 units and a 3x3 kernel, then a 64 units layer also with a 3x3 kernel, followed by a `MaxPooling2D` with a 2xs pool size layer and a 25% `Dropout`. The output is then `Flatten`ed and connected to a `Dense` layer of 128 neurons, another 50% `Dropout` and then a single neuron with a `sigmoid` activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "img_rows, img_cols, channels = 100, 100, 3\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, channels)\n",
    "x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, channels)\n",
    "input_shape = (img_rows, img_cols, channels)\n",
    "\n",
    "### your code here ###\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    shuffle=True,\n",
    "                    validation_data=(x_test, y_test))\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=1)\n",
    "\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got a test accuracy of ~84%, which is a great improvement to what we were able to achieve before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc = 'upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q8)__ how many weights does this network need to learn?<br>\n",
    "__Q9)__ what is the AUC of this model?<br>\n",
    "__Q10)__ what is the avg F1 score of this model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans['Q8'] = #### your answer here ####\n",
    "ans['Q9'] = #### your answer here ####\n",
    "ans['Q10'] = #### your answer here ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bonus\n",
    "\n",
    "play with the neural network hyper-parameters (depth, # nodes, activation function, batch size, # epochs etc.)<br>\n",
    "report your models AUC.<br>\n",
    "you will get 5 points for beating the auc of the previous CNN model<br>\n",
    "and 5 extra points for the student with the best AUC in class.<br>\n",
    "__do not__ change the random seed values.<br>\n",
    " Notice you have many parameters to tune and also have a look at [`EarlyStopping`](https://keras.io/callbacks/#earlystopping) callback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "report your best auc score<br> \n",
    "__copy your model code as string__ (we will reporoduce your settings to validate the result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans['bonus'] = ### auc score ###\n",
    "ans['bonus_model'] = ### model as string! ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# finish!\n",
    "\n",
    "to submit your HW please run this last code block and follow the instructions. <BR>\n",
    "this code will create a CSV file in the current directory on the azure notebooks project <br>\n",
    "please download it and submit it through moodle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_ans = pd.DataFrame.from_dict(ans, orient='index')\n",
    "if df_ans.shape[0] == 12 or df_ans.shape[0] == 14:\n",
    "    df_ans.to_csv('{}_{}.csv'.format(ans['HW'],str(ans['id_number'])))\n",
    "    print(\"OK!\")\n",
    "else:\n",
    "    print(\"seems like you missed a question, make sure you have run all the code blocks\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
